#!/usr/bin/env python3
import csv
import re
from pathlib import Path
from typing import Dict, List, Set, Tuple

# ============================
# CONFIG (AUCUN ARGUMENT)
# ============================
BASE_DIR = Path(__file__).parent.resolve()
LOGS_DIR = BASE_DIR / "logs"

# 1) CSV dÃ©doublonnÃ©
OUTPUT_UNIQUE = BASE_DIR / "hls_unique.csv"
# 2) CSV complet (toutes occurrences, comme quand tu listes "ligne par ligne")
OUTPUT_ALL = BASE_DIR / "hls_all.csv"

# ============================
# REGEX ROBUSTE
# ============================
# Capture les HLS au format HLS + 8 digits, mÃªme si sÃ©parÃ©s par :
# espace, :, _, -, =, etc. + accepte ponctuation aprÃ¨s le nombre
#
# Match:
# - HLS00372942
# - HLS 00372942
# - HLS:00372942
# - HLS_00372942
# - HLS-00372942
# - HLS=00372942
# - HLS=00372942,
# - (HLS00372942)
HLS_REGEX = re.compile(r"(?i)\bHLS\b[\s:_\-=]*([0-9]{8})(?![0-9])")

def list_all_files(folder: Path) -> List[Path]:
    return [p for p in folder.rglob("*") if p.is_file()]

def extract_from_file(file_path: Path) -> Tuple[List[str], int]:
    """
    Retourne:
      - liste de HLS trouvÃ©s (avec doublons) sous forme canonique HLS########
      - nb de lignes lues
    """
    found_all: List[str] = []
    lines_read = 0

    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            lines_read += 1
            for m in HLS_REGEX.finditer(line):
                found_all.append("HLS" + m.group(1))

    return found_all, lines_read

def write_csv_unique(path: Path, values: List[str]) -> None:
    unique_sorted = sorted(set(values))
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["HLS"])
        for v in unique_sorted:
            w.writerow([v])

def write_csv_all(path: Path, values: List[str]) -> None:
    # liste brute, comme "mes 400 lignes"
    with open(path, "w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["HLS"])
        for v in values:
            w.writerow([v])

def main():
    if not LOGS_DIR.exists() or not LOGS_DIR.is_dir():
        print(f"âŒ Dossier logs introuvable : {LOGS_DIR}")
        print("âž¡ï¸ CrÃ©e un dossier 'logs' Ã  cÃ´tÃ© de ce script et mets tous tes fichiers dedans.")
        return

    files = list_all_files(LOGS_DIR)
    if not files:
        print(f"âŒ Aucun fichier trouvÃ© dans : {LOGS_DIR}")
        return

    all_hls: List[str] = []
    total_lines = 0
    files_with_hits = 0
    per_file_hits: Dict[str, int] = {}

    for fp in files:
        values, lines = extract_from_file(fp)
        total_lines += lines
        all_hls.extend(values)

        hits = len(values)
        per_file_hits[str(fp)] = hits
        if hits > 0:
            files_with_hits += 1

    # Export
    write_csv_all(OUTPUT_ALL, all_hls)
    write_csv_unique(OUTPUT_UNIQUE, all_hls)

    # Stats
    total_occ = len(all_hls)
    total_unique = len(set(all_hls))

    print("\n================ RÃ‰SULTAT ================")
    print(f"ðŸ“‚ Fichiers analysÃ©s          : {len(files)}")
    print(f"ðŸ“„ Fichiers contenant des HLS : {files_with_hits}")
    print(f"ðŸ“‘ Lignes lues                : {total_lines}")
    print(f"ðŸ”Ž Occurrences HLS (brut)     : {total_occ}")
    print(f"âœ… HLS uniques                : {total_unique}")
    print(f"ðŸ’¾ CSV complet (brut)         : {OUTPUT_ALL}")
    print(f"ðŸ’¾ CSV dÃ©doublonnÃ©            : {OUTPUT_UNIQUE}")
    print("==========================================\n")

    # Top 10 fichiers qui contiennent le plus de HLS (diagnostic)
    top = sorted(per_file_hits.items(), key=lambda x: x[1], reverse=True)[:10]
    print("Top fichiers (HLS trouvÃ©s) :")
    for path, cnt in top:
        if cnt > 0:
            print(f"  - {cnt:6d}  {path}")

if __name__ == "__main__":
    main()