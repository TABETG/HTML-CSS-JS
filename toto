#!/usr/bin/env python3
import argparse
import csv
import re
from pathlib import Path
from typing import Iterable, Set, Tuple

# ✅ Matche :
# - HLS00372942
# - HLS 00372942
# - HLS:00372942
# - HLS_00372942
# - HLS-00372942
#
# On reconstruit ensuite au format canonique "HLS" + 8 digits => ex: HLS00372942
HLS_REGEX = re.compile(r"\bHLS\b[\s:_-]*([0-9]{8})\b", re.IGNORECASE)


def iter_files(input_dir: Path, extensions: Tuple[str, ...]) -> Iterable[Path]:
    # ✅ Par défaut: on parcourt TOUT, y compris sous-dossiers
    for p in input_dir.rglob("*"):
        if not p.is_file():
            continue

        if not extensions:
            yield p
        else:
            # ✅ extension case-insensitive (.log, .LOG, etc.)
            if p.suffix.lower() in extensions:
                yield p


def extract_hls_from_file(path: Path) -> Tuple[Set[str], int, int]:
    """
    Retourne:
      - set des HLS uniques trouvés dans ce fichier
      - nb d'occurrences matchées (avec doublons)
      - nb de lignes lues
    """
    found: Set[str] = set()
    matches_count = 0
    lines_count = 0

    # ✅ lecture ligne par ligne, robuste aux encodages
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        for line in f:
            lines_count += 1
            for m in HLS_REGEX.finditer(line):
                matches_count += 1
                found.add("HLS" + m.group(1))  # format canonique

    return found, matches_count, lines_count


def write_csv(output_csv: Path, hls_values: Iterable[str]) -> None:
    output_csv.parent.mkdir(parents=True, exist_ok=True)
    with output_csv.open("w", newline="", encoding="utf-8") as f:
        w = csv.writer(f)
        w.writerow(["HLS"])
        for hls in hls_values:
            w.writerow([hls])


def main():
    parser = argparse.ArgumentParser(description="Extract HLS######## from all log files in a folder and export unique list to CSV.")
    parser.add_argument("-i", "--input", required=True, help="Dossier contenant les logs")
    parser.add_argument("-o", "--output", default="hls_unique.csv", help="Chemin du CSV de sortie")
    parser.add_argument(
        "--ext",
        nargs="*",
        default=[],
        help="Extensions à lire (ex: .log .txt). Par défaut: lit TOUS les fichiers.",
    )
    parser.add_argument("--sort", action="store_true", help="Trie les HLS avant export")
    args = parser.parse_args()

    input_dir = Path(args.input).expanduser().resolve()
    output_csv = Path(args.output).expanduser().resolve()

    if not input_dir.exists() or not input_dir.is_dir():
        raise SystemExit(f"❌ Dossier introuvable: {input_dir}")

    # ✅ Par défaut: aucune extension => lire tous les fichiers
    if args.ext and len(args.ext) > 0:
        extensions = []
        for e in args.ext:
            e = e.strip().lower()
            if e and not e.startswith("."):
                e = "." + e
            if e:
                extensions.append(e)
        ext_tuple: Tuple[str, ...] = tuple(extensions)
    else:
        ext_tuple = ()

    files = list(iter_files(input_dir, ext_tuple))
    if not files:
        raise SystemExit("❌ Aucun fichier trouvé dans le dossier (vérifie le chemin et/ou --ext).")

    all_unique: Set[str] = set()
    total_occurrences = 0
    total_lines = 0
    files_with_hits = 0

    for fp in files:
        uniq, occ, lines = extract_hls_from_file(fp)
        total_lines += lines
        total_occurrences += occ
        if occ > 0:
            files_with_hits += 1
        all_unique.update(uniq)

    export_list = sorted(all_unique) if args.sort else list(all_unique)
    write_csv(output_csv, export_list)

    print(f"✅ Fichiers parcourus: {len(files)} (dont {files_with_hits} avec au moins 1 HLS)")
    print(f"✅ Lignes lues: {total_lines}")
    print(f"✅ Occurrences HLS trouvées (avec doublons): {total_occurrences}")
    print(f"✅ HLS uniques: {len(all_unique)}")
    print(f"✅ CSV généré: {output_csv}")


if __name__ == "__main__":
    main()